---
title: "Bayesian model for COVID IFR"
author: "Witold Wiecek for 1 Day Sooner"
date: "Last updated `r Sys.Date()`"
output: pdf_document
bibliography: bibliography.bib
---

```{r setup, include=FALSE}
knitr::opts_chunk$set(echo = FALSE)
library(baggr)
library(tidyverse)
library(readxl)
options(digits = 2)
```

# Introduction

This is a short document describing a Bayesian model for synthesising information on many infection fatality rates (IFRs) into a single estimate that can be made specific to certain age groups or adjusted by co-morbidity status. The analysis presented here is a form of Bayesian meta-analysis, in that our primary objective is to weigh sources of evidence in a way that captures both variability (heterogeneity across different settings) and uncertainty.

Our ultimate objective is to characterise risks in a particular setting, population and time, in a way that is useful to understanding risks of human challenge trials (HCTs). Therefore, as a minimum, we want to incorporate variability into our prediction. Even better would be to understand how different factors can drive heterogeneity. Indeed, _a priori_, we can hypothesise that the three main drivers of differences in IFRs are time-specific, population-specific and otherwise country-specific. 

The role of time may be due to new treatments, improvements over time in our ability to treat Covid-19 or selection pressures which may lead to more benign versions of the virus.
Country-specific or location-specific factors in IFR data may be driven by under-reporting, health care factors (including access to health care services) or underlying distributions of known risk factors. Additionally, some unknown risk factors (e.g. genetic) may also be operating, in which case controlling for age and co-morbidities will be not sufficient to account for cross-location differences. 

To address these drivers of differences in observed IFRs we develop a Bayesian model and apply it to publicly available summary data on IFRs from multiple countries and contexts, with particular focus on the impact of age.



# Methods

## Bayesian model for evidence synthesis

What follows is an adaptation of typical methods of Bayesian evidence synthesis to analysis of IFRs. IFR is a proportion statistic, calculated as the ratio of deaths to infections in some population. Early estimates, e.g. by @verity_estimates_2020, place it at over 0.6% globally. However, the risk of death is orders of magnitude higher in particular high risk groups, especially in the elderly, than in the general population.

We can use Bayesian models for repeated binary trials, accounting for the fact that different populations studies at different times have different average probability of events. We use hierarchical modelling framework to assume that the context-specific estimates of $IFR_i$ (measured in different settings, with some uncertainty) are all linked using some common parameters.

The most straight-forward and "canonical" ways to implement such a Bayesian model is by modelling log odds of the event.^[It is also possible to work with $IFR_i$ parameters and treat them as derived from Beta distribution with some "hyperparameters" $\alpha$ and $\beta$ of Beta distribution, as done by e.g. @carpenter_hierarchical_2016. That approach, however, does not offer an easy way of modelling impact of covariates (e.g. age and co-morbidities) on the rates.]
@deeks_issues_2002 present a general treatment. Note, that for very rare events the odds of mortality are very similar to probability of mortality, but we model events on odds scale as a good "generic" approach to modelling binary data (in this case death following infections). Another advantage of such a model is that it can use either individual-level or summary data and work with covariates (such as gender, age, time of the study, co-morbidities), captured as odds ratios or risk ratios^[If only summary data are available, covariates can be defined as study level distributions (e.g. % male)]. 

Basic models for analysis of binary data can be implemented using existing statistical analysis packages (see, for example, _baggr_  by @wiecek_baggr_2020), by treating IFR as a logit-normal parameter to meta-analyse. However, note that when no deaths are observed, analysis of the ratio statistic that is IFR (ratio of observed deaths to modelled infections) is problematic. Therefore we propose a "custom" model that built in Stan which treats deaths and _prevalences_ as data (rather than the IFRs). 

Let $d_k$ denote observed deaths for data point $k$ and assume that logit of prevalence $p_k$ in the population of $n_k$ subjects is obtained from some model. We can then write:

$$
d_k \sim \text{Binomial}(n_k, p_kIFR_k) \\
\text{logit}(p_k) \sim \mathcal{N}(\mu^{(p)}_k, \sigma^{(p)}_k)
$$
where $\sigma^{(p)}_k$ and $\mu^{(p)}_k$ are parameters derived from the existing models of prevalence.

The $k$ data points collected can span many locations (studies); we denote them by $\text{loc}_k$ and the total number of locations by $N_{loc}$
We can also collect other covariates impacting the IFRs, such as age groups (which we identify with median age of the population being studied, $\text{MedianAge}_k$). We denote all of the covariates using a design matrix $X$ and denote by $N_p$ the number of columns in $X$. 
We assume the impact on IFR is on logit scale, same as in the "canonical" logistic models of binary data that we mentioned above:

$$
\text{logit}(IFR_k) = \theta_{\text{loc}_k} + X\beta
$$
where $\theta$ is an $N_{loc}$-dimensional vector of location-specific (random) effects on IFR and $\beta$ is $N_p$ dimensional vector of (fixed) covariate effects.

We implement our model in Stan and assume mildly regularising priors on all parameters:

```
model {
  //Likelihood:
  logit_prevalence ~ normal(mean_prevalence, sd_prevalence);
  obs_deaths ~ binomial(population, prevalence .* ifr);
  theta_k ~ normal(tau, sigma);

  //Priors:
  tau   ~ normal(0, 100);
  sigma ~ normal(0, 10);
  beta  ~ normal(0, 10);
}
```







## Model data

We use estimates originally collected by @levin_assessing_2020 to construct the first version of analysis dataset. The input data into our model consists of deaths (treated as known) and prevalences (treated as logit-distributed parameter with known mean and SD) in all reported age groups in all studies^[This basic approach exaggerates uncertainty, as we treat different 95% intervals reported in the study as uncorrelated.].

```{r, include = F}
# Several different datasets.
NBER_IFR_Benchmark_Studies <- read_excel('Study Summary Data/NBER_IFR_Meta_Dataset.xlsx',1)
#Places: Belgium. Geneva, Indiana, New York, Spain, Sweden
NBER_IFR_US_Studies <- read_excel('Study Summary Data/NBER_IFR_Meta_Dataset.xlsx',2, skip=1)
names(NBER_IFR_US_Studies)[6:7]<-c('Infect 95_lower','Infect 95_upper')
names(NBER_IFR_US_Studies)[10:11]<-c('IFR_95_lower','IFR_95_upper')

# NBER_All_Studies <- read_excel('Study Summary Data/NBER_IFR_Meta_Dataset.xls',5)
```

```{r}
logit <- function(x) log(x/(1-x))
inv_logit <- function(x) exp(x)/(1+exp(x))

ifr_global <- NBER_IFR_Benchmark_Studies %>%
  group_by(Study) %>%
  mutate(ir = InfectionRate/100, 
         ir_low = infrate_ci95_low/100, 
         ir_high = infrate_ci95_high/100) %>%
  select(Study, AgeGroup, Median_Age, Deaths, Population, ir, ir_low, ir_high) %>%
  mutate(dataset = "Global")

ifr_us <- NBER_IFR_US_Studies %>%
  filter(!is.na(AgeGroup)) %>%
  group_by(Study) %>%
  mutate(ir = `Infection Rate (%)`/100, 
         ir_low = `Infect 95_lower`/100, 
         ir_high = `Infect 95_upper`/100) %>%
  select(Study, AgeGroup, Median_Age, Deaths, Population, ir, ir_low, ir_high) %>%
  mutate(dataset = "United States")
```

```{r prevalence-data, fig.cap = "Distribution of model-estimated prevalences (95% CI's reported by modelling studies) collected by @levin_assessing_2020. Additional points show 95% CIs recreated by assuming logit-normal distribution of prevalence."}
rbind(ifr_global, ifr_us) %>%
  mutate(log_sd = ifelse(ir_low != 0, 
                         (logit(ir_high) - logit(ir_low))/(2*1.96), 
                         (logit(ir_high) - logit(ir))/1.96)) %>%
  mutate(log_mean = logit(ir)) %>%
  mutate(midpoint_u = log_mean + log_sd*1.96, midpoint_l = log_mean - log_sd*1.96) %>%
  ggplot(aes(x=ir, xmax = ir_high, xmin=ir_low, y=interaction(Study, AgeGroup))) + 
  geom_point() + geom_errorbarh() +
  geom_point(aes(x = inv_logit(midpoint_l)), pch = 21) +
  geom_point(aes(x = inv_logit(midpoint_u)), pch = 21)

```

```{r}


df <- rbind(ifr_global, ifr_us) %>%
  mutate(logit_sd = ifelse(ir_low != 0, 
                         (logit(ir_high) - logit(ir_low))/(2*1.96), 
                         (logit(ir_high) - logit(ir))/1.96)) %>%
  mutate(logit_mean = logit(ir))

mm <- model.matrix(ir ~ Study + Median_Age, data = df)
stan_data <- list(
  X = matrix(df$Median_Age/10 - 2.5, nrow(df), 1),
  N = nrow(df), 
  Np = 1,
  Nloc = length(unique(df$Study)),
  loc = as.numeric(as.factor(df$Study)),
  mean_prevalence = df$logit_mean,
  sd_prevalence = df$logit_sd,
  population = df$Population,
  obs_deaths = df$Deaths)
```


```{r}
library(rstan)
rstan_options(auto_write = TRUE)
sm <- stan_model("ifr_with0.stan")
options(mc.cores = 4)
```

```{r, cache = TRUE}
fit <- sampling(sm, data = stan_data, control = list(max_treedepth = 15), refresh = 0)
```

```{r}
print(fit, c("tau", "sigma", "beta"))
```

## Average infection fatality risk in young subjects

```{r}
hypermean <- (rstan::extract(fit, "tau")[[1]])
hypersd <- (rstan::extract(fit, "sigma")[[1]])

hyperifr <- inv_logit(hypermean)
replications <- inv_logit(rnorm(4000, hypermean, hypersd))
```

Since we use a transformation `MedianAge/10 - 2.5` to construct our matrix $X$, we can obtain model-estimated risk for a typical HCT population (aged 20 to 30, with median 25) by ignoring the $\beta$ coefficient and examining $\tau$ and $\sigma$ only. We find that the average IFR for this group (equal to $\frac{\exp{(\tau)}}{\exp{(1 + \tau)}}$) is `r mean(hyperifr)` (with 95% interval from `r quantile(hyperifr, .025)` to `r quantile(hyperifr, .975)`). That means slightly under 2 deaths per 10,000 infections in the studied datasets.


## Heterogeneity in IFRs

There is a considerable variability in IFRs across different locations/dataset. To take into account parameter $\sigma$, we can generate draws from the $\mathcal{N}(\tau, \sigma^2)$ distribution, corresponding to a hypothetical IFR in a new source of data. 95% interval for such model runs from `r quantile(replications, .025)` to `r quantile(replications, .975)`. Since the model works a logistic scale, another way of interpreting the across-dataset variability is asking the impact of $\sigma$ on the mean IFR; here, we obtain on average a `r mean(exp(2*hypersd))`-fold increase (decrease) in IFR per $2\sigma$ increase (decrease).

The lower end of the 95% interval, `r quantile(replications, .025)`, is not extreme given input data, where the "crude" mean IFR (based on mean prevalence only) is below 7 per 10,000 for all data, except for South Florida, and as low as 1.4 per 10,000 in Utah, in population aged 19-44.

```{r}
df %>% filter(Median_Age > 10, Median_Age < 35) %>% 
  select(Study, AgeGroup, Deaths, Population, ir) %>%
  mutate(crude_ifr = Deaths/(Population*ir))
```
```{r}
theta_mean <- summary(fit, "theta_k")$summary[,"mean"]
names(theta_mean) <- levels(as.factor(df$Study))

```

We can assess this heterogeneity by inspecting the distribution of random effects in the model transformed into IFRs, 
i.e. the inverse logit transformation $\theta$ parameters. 
The largest (posterior mean) IFR value of $\theta$ is `r inv_logit(max(theta_mean))` in `r names(theta_mean)[which.max(theta_mean)]`.
The smallest posterior mean is `r inv_logit(min(theta_mean))` in `r names(theta_mean)[which.min(theta_mean)]`.




## Predictive checks for the model

```{r}
df_cov <- data.frame(study = df$Study, 
           observed = df$Deaths,
           summary(fit, "ppc_deaths")$summary) %>% 
  mutate(coverage975 = (observed + .1) > X2.5. & observed < X98.) %>%
  mutate(coverage50 = (observed + .1) > X25. & observed < X75.) %>% 
  summarise(n = n(), coverage975 = sum(coverage975), coverage50 = sum(coverage50))
```

We constructed posterior predictive distributions for number of deaths in each of the inputs by using the `generated quantities` functionality of Stan. 
Out of `r df_cov$n` observations that were used to fit the model, `r df_cov$coverage975` were within 95% intervals of the 
posterior predictive distributions, with 3 discrepancies occuring in Spanish data. 
This suggests that the simple binomial model we used here is flexible enough to capture both age-specific risk increases and differences across countries.


Plot of the ppc 95% intervals vs observed N deaths

```{r}
data.frame(study = df$Study, 
           age = df$AgeGroup,
           observed = df$Deaths,
           summary(fit, "ppc_deaths")$summary) %>% 
  mutate_if(is.numeric, function(x) x/df$Population) %>%
  mutate(deaths_cut = cut(observed, c(-Inf, 1/1e06, 10/1e06, 100/1e06, Inf))) %>%
  ggplot(aes(y=interaction(study, age))) + 
  geom_point(aes(x = mean)) + 
  geom_errorbarh(aes(xmin = X2.5., xmax = X98.)) + 
  geom_point(aes(x = observed), color = "red") + 
  facet_wrap(~deaths_cut, ncol = 2, scales = "free")
```

# What is the risk in healthy individuals?


```{r}
open_safely <- read_excel("Study Summary Data/OpenSafely_Counts.xls")

stan_df <- open_safely %>% 
  filter(!is.na(Fatalities)) %>% 
  mutate(f = ifelse(Fatalities == "<=5", "5", Fatalities)) %>%
  mutate(f = as.numeric(f)) %>%
  mutate(p = f/Population) 
```

Data for this section has been provided by OpenSAFELY (_https://opensafely.org/_) and is as described by @williamson_factors_2020 in a recent article on Covid-19 mortality risk factors for 10,926 Covid-19 deaths in England. We group the total of 21,444,863 individuals into high and low risk groups as follows:

__Definition of co-morbiditiy goes here__

In contrast to the cited publication, we include records of individuals under 18 in our assessment.

```{r}

como_df <- stan_df %>%
  filter(`Healthy Weight` == 0) %>%
  group_by(Age_min, `No Comorbidities`) %>%
  summarise(p=sum(Population), f=sum(f)) %>%
  mutate(m = f/p) 

como_df %>%
  rename(status = `No Comorbidities`) %>%
  mutate(status = ifelse(status, 
                                "No comorbidities", "All individuals")) %>%
  ggplot(aes(x = Age_min + 5, y=log(m), 
             group = status, color = status)) +
  geom_point() +
  geom_line() +
    xlab("Age") + ylab("log(Pr of death)") +
  theme(legend.position = "bottom")

como_df %>%
  select(-f, -p) %>%
  spread(`No Comorbidities`, m) %>%
  setNames(c("age_group_min", "all_risk", "healthy_risk")) %>%
  mutate(rr = healthy_risk/all_risk)
```



```{r, eval = T}
stan_df %>%
  ggplot(aes(x = Age_min, y = log(p), color = Gender)) + geom_point() + facet_wrap(~ `No Comorbidities`)

stan_data <- list(
  N = nrow(stan_df),
  age = ifelse(stan_df$Age_min >= 20, (stan_df$Age_min - 20)/10, 0), #experimental
  male = 1*(stan_df$Gender == "M"),
  comorbidities = 1*(stan_df$`No Comorbidities` == 0),
  overweight = 1*(stan_df$`Healthy Weight` == 0),
  under20 = 1*(stan_df$Age_min < 20),
  Npop = stan_df$Population,
  Nevents = stan_df$f)

sm_como <- stan_model("rr_comorbidities.stan")
```

```{r, cache = TRUE}
fit_como <- sampling(sm_como, data = stan_data, refresh = 0)
```


```{r}
data.frame(
  stan_df,
  lci = summary(fit_como, "theta")$summary[,"2.5%"],
  estimate = summary(fit_como, "theta")$summary[,"mean"],
  uci = summary(fit_como, "theta")$summary[,"98%"]) %>%
  mutate(lci = log(lci), estimate = log(estimate), uci = log(uci)) %>%
  ggplot(aes(x = Age_min, y = log(p), color = Gender)) + 
  geom_point(pch = 21) +
  geom_point(aes(y = estimate), pch = 2) +
  geom_errorbar(aes(ymin = lci, ymax = uci)) +
  facet_grid(Healthy.Weight ~ No.Comorbidities)
```


# References
