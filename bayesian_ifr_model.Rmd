---
title: "Bayesian model for COVID IFR"
author: "Witold Wiecek for 1 Day Sooner"
date: "Last updated `r Sys.Date()`"
output: pdf_document
bibliography: bibliography.bib
---

```{r setup, include=FALSE}
knitr::opts_chunk$set(echo = FALSE)
library(baggr)
library(tidyverse)
library(readxl)
```

# Introduction

This is a short document describing a Bayesian model for synthesising information on many infection fatality rates (IFRs) into a single estimate that can be made specific to certain age groups or adjusted by co-morbidity status. The analysis presented here is a form of Bayesian meta-analysis, in that our primary objective is to weigh sources of evidence in a way that captures both variability (heterogeneity across different settings) and uncertainty.

Our ultimate objective is to characterise risks in a particular setting, population and time, in a way that is useful to understanding risks of human challenge trials (HCTs). Therefore, as a minimum, we want to incorporate variability into our prediction. Even better would be to understand how different factors can drive heterogeneity. Indeed, _a priori_, we can hypothesise that the three main drivers of differences in IFRs are time-specific, population-specific and otherwise country-specific. 

The role of time may be due to new treatments, improvements over time in our ability to treat Covid-19 or selection pressures which may lead to more benign versions of the virus.
Country-specific or location-specific factors in IFR data may be driven by under-reporting, health care factors (including access to health care services) or underlying distributions of known risk factors. Additionally, some unknown risk factors (e.g. genetic) may also be operating, in which case controlling for age and co-moribidities will be not sufficient to account for cross-location differences. 

To address these drivers of differences in observed IFRs we develop a Bayesian model and apply it to publicaly available summary data on IFRs from multiple countries and contexts, with particular focus on the impact of age.



# Methods

## Bayesian model for evidence synthesis

What follows is an adaptation of typical methods of Bayesian evidence synthesis to analysis of IFRs. IFR is a proportion statistic, calculated as the ratio of deaths to infections in some population. Early estimates, e.g. by @verity_estimates_2020, place it at over 0.6% globally. However, the risk of death is orders of magnitude higher in particular high risk groups, especially in the elderly, than in the general population.

We can use Bayesian models for repeated binary trials, accounting for the fact that different populations studies at different times have different average probability of events. We use hierarchical modelling framework to assume that the context-specific estimates of $IFR_i$ (measured in different settings, with some uncertainty) are all linked using some common parameters.

The most straight-forward and "canonical" ways to implement such a Bayesian model is by modelling log odds of the event.^[It is also possible to work with $IFR_i$ parameters and treat them as derived from Beta distribution with some "hyperparameters" $\alpha$ and $\beta$ of Beta distribution, as done by e.g. @carpenter_hierarchical_2016. That approach, however, does not offer an easy way of modelling impact of covariates (e.g. age and co-morbidities) on the rates.]
@deeks_issues_2002 present a general treatment. Note, that for very rare events the odds of mortality are very similar to probability of mortality, but we model events on odds scale as a good "generic" approach to modelling binary data (in this case death following infections). Another advantage of such a model is that it can use either individual-level or summary data and work with covariates (such as gender, age, time of the study, co-morbidities), captured as odds ratios or risk ratios^[If only summary data are available, covariates can be defined as study level distributions (e.g. % male)]. 

Basic models for analysis of binary data can be implemented using existing statistical analysis packages (see, for example, _baggr_  by @wiecek_baggr_2020), by treating IFR as a logit-normal parameter to meta-analyse. However, note that when no deaths are observed, analysis of the ratio statistic that is IFR (ratio of observed deaths to modelled infections) is problematic. Therefore we propose a "custom" model that built in Stan which treats deaths and _prevalences_ as data (rather than the IFRs). 

Let $d_k$ denote observed deaths for data point $k$ and assume that logit of prevalence $p_k$ in the population of $n_k$ subjects is obtained from some model. We can then write:

$$
d_k \sim \text{Binomial}(n_k, p_kIFR_k) \\
\text{logit}(p_k) \sim \mathcal{N}(\mu^{(p)}_k, \sigma^{(p)}_k)
$$
where $\sigma^{(p)}_k$ and $\mu^{(p)}_k$ are parameters derived from the existing models of prevalence.

The $k$ data points collected can span many locations (studies); we denote them by $\text{loc}_k$ and the total number of locations by $N_{loc}$
We can also collect other covariates impacting the IFRs, such as age groups (which we identify with median age of the population being studied, $\text{MedianAge}_k$). We denote all of the covariates using a design matrix $X$ and denote by $N_p$ the number of columns in $X$. 
We assume the impact on IFR is on logit scale, same as in the "canonical" logistic models of binary data that we mentioned above:

$$
\text{logit}(IFR_k) = \theta_{\text{loc}_k} + X\beta
$$
where $\theta$ is an $N_{loc}$-dimensional vector of location-specific (random) effects on IFR and $\beta$ is $N_p$ dimensional vector of (fixed) covariate effects.

We implement our model in Stan and assume mildly regularising priors on all parameters:

```
model {
  //Likelihood:
  logit_prevalence ~ normal(mean_prevalence, sd_prevalence);
  obs_deaths ~ binomial(population, prevalence .* ifr);
  theta_k ~ normal(tau, sigma);

  //Priors:
  tau   ~ normal(0, 10);
  sigma ~ normal(0, 10);
  beta  ~ normal(0, 10);
}
```







## Model data

We use estimates originally collected by @levin_assessing_2020 to construct the first version of analysis dataset. The input data into our model consists of deaths (treated as known) and prevalences (treated as logit-distributed parameter with known mean and SD) in all reported age groups in all studies^[This basic approach exaggerates uncertainty, as we treat different 95% intervals reported in the study as uncorrelated.].

```{r, include = F}
# Several different datasets.
NBER_IFR_Benchmark_Studies <- read_excel('Study Summary Data/NBER_IFR_Meta_Dataset.xlsx',1)
#Places: Belgium. Geneva, Indiana, New York, Spain, Sweden
NBER_IFR_US_Studies <- read_excel('Study Summary Data/NBER_IFR_Meta_Dataset.xlsx',2, skip=1)
names(NBER_IFR_US_Studies)[6:7]<-c('Infect 95_lower','Infect 95_upper')
names(NBER_IFR_US_Studies)[10:11]<-c('IFR_95_lower','IFR_95_upper')

# NBER_All_Studies <- read_excel('Study Summary Data/NBER_IFR_Meta_Dataset.xls',5)
```

```{r}
logit <- function(x) log(x/(1-x))
inv_logit <- function(x) exp(x)/(1+exp(x))

ifr_global <- NBER_IFR_Benchmark_Studies %>%
  group_by(Study) %>%
  mutate(ir = InfectionRate/100, 
         ir_low = infrate_ci95_low/100, 
         ir_high = infrate_ci95_high/100) %>%
  select(Study, AgeGroup, Median_Age, Deaths, Population, ir, ir_low, ir_high) %>%
  mutate(dataset = "Global")

ifr_us <- NBER_IFR_US_Studies %>%
  filter(!is.na(AgeGroup)) %>%
  group_by(Study) %>%
  mutate(ir = `Infection Rate (%)`/100, 
         ir_low = `Infect 95_lower`/100, 
         ir_high = `Infect 95_upper`/100) %>%
  select(Study, AgeGroup, Median_Age, Deaths, Population, ir, ir_low, ir_high) %>%
  mutate(dataset = "United States")
```

```{r prevalence-data, fig.cap = "Distribution of model-estimated prevalences (95% CI's reported by modelling studies) collected by @levin_assessing_2020. Additional points show 95% CIs recreated by assuming logit-normal distribution of prevalence."}
rbind(ifr_global, ifr_us) %>%
  mutate(log_sd = ifelse(ir_low != 0, 
                         (logit(ir_high) - logit(ir_low))/(2*1.96), 
                         (logit(ir_high) - logit(ir))/1.96)) %>%
  mutate(log_mean = logit(ir)) %>%
  mutate(midpoint_u = log_mean + log_sd*1.96, midpoint_l = log_mean - log_sd*1.96) %>%
  ggplot(aes(x=ir, xmax = ir_high, xmin=ir_low, y=interaction(Study, AgeGroup))) + 
  geom_point() + geom_errorbarh() +
  geom_point(aes(x = inv_logit(midpoint_l)), pch = 21) +
  geom_point(aes(x = inv_logit(midpoint_u)), pch = 21)

```

```{r}


df <- rbind(ifr_global, ifr_us) %>%
  mutate(logit_sd = ifelse(ir_low != 0, 
                         (logit(ir_high) - logit(ir_low))/(2*1.96), 
                         (logit(ir_high) - logit(ir))/1.96)) %>%
  mutate(logit_mean = logit(ir))

mm <- model.matrix(ir ~ Study + Median_Age, data = df)
stan_data <- list(
  X = matrix(df$Median_Age/10 - 2.5, nrow(df), 1),
  N = nrow(df), 
  Np = 1,
  Nloc = length(unique(df$Study)),
  loc = as.numeric(as.factor(df$Study)),
  mean_prevalence = df$logit_mean,
  sd_prevalence = df$logit_sd,
  population = df$Population,
  obs_deaths = df$Deaths)
```


```{r}
library(rstan)
rstan_options(auto_write = TRUE)
sm <- stan_model("ifr_with0.stan")
options(mc.cores = 4)
```

```{r, cache = TRUE}
fit <- sampling(sm, data = stan_data, control = list(max_treedepth = 15))
```

```{r}
print(fit, c("tau", "sigma", "beta"))
```


## Predictive checks for the model

...

# References
